---
title: "Models"
author: "Lukáš Častven, Michal Kilian"
date: "`r Sys.Date()`"
output: html_document
---

Based on these EDA findings, as our first model, we choose simple decision trees.
This model choice is supported by the EDA which identified some non-linear relationships
(e.g., the threshold effect of securing `first_` objectives) and mixed data types
(numeric `_diff`, `_champdiff`, `time_first_*`, categorical `first_*`). While features
like `time_first_*` and `_champdiff` showed limited individual predictive power.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(tidyverse)
library(tidymodels)
library(rpart)
library(rpart.plot)
library(caret)
library(ranger)
library(adabag)
library(patchwork)
source("./dataset.R")
source("./model_comparison.R")

N <- 12 * 60
dataset <- early_game_dataset(player_stats, metadata, events, champs, N) %>%
	mutate(team1_won = factor(team1_won))

set.seed(42069)
train_indices <- createDataPartition(dataset$team1_won, p = 0.75, list = FALSE, times = 1)
train_data <- dataset[train_indices, ]
test_data <- dataset[-train_indices, ]

color1 <- "plum"
color2 <- "#A0DDA1"
```

## Decision trees

We first try all features that shows promise in EDA (`first_` and diffs).

```{r}
formula <- team1_won ~ kill_diff + dragon_diff + rift_herald_diff +
  tower_diff + first_blood + first_dragon + first_herald +
  first_tower

tree_model <- rpart(
	formula = formula,
	data = train_data,
	method = "class",
)

plot_decision_tree(tree_model)
```

The default `rpart` tree has `cp = 0.01`, and this configuration chooses
features `kill_diff` and `dragon_diff`. Comparison of the two models:

```{r}
predictions <- predict(tree_model, newdata = train_data, type = "class")
train_conf_matrix <- confusionMatrix(data = predictions, reference = train_data$team1_won, positive = "1")

predictions <- predict(tree_model, newdata = test_data, type = "class")
test_conf_matrix <- confusionMatrix(data = predictions, reference = test_data$team1_won, positive = "1")

compare_conf_matrices(train_conf_matrix, test_conf_matrix, c("training", "testing"))
```

There isn't a significant difference in training vs testing dataset.
The model is good when team 1 wins (sensitivity), it correctly identifies 75%
games where team 1 won. But it isn't good at identifying when team 1 looses,
only 57.7% of games where team 1 lost were predicted as such.

```{r}
roc(tree_model, test_data)
```

The ROC curve shows that the model is better than a random classifier.

### Analysis of FPs

```{r}
predictions <- tibble(
  game_id = test_data$game_id,
  actual = test_data$team1_won,
  predicted = predict(tree_model, newdata = test_data, type = "class")) %>%
  left_join(test_data, by = "game_id")


# False Positives (predicted=1, actual=0)
fp_games <- predictions %>% filter(predicted == "1" & actual == "0")
# True Negatives (predicted=0, actual=0) - Why weren't FP games like these?
tn_games <- predictions %>% filter(predicted == "0" & actual == "0")
# True Positives (predicted=1, actual=1) - What made FP games look like these?
tp_games <- predictions %>% filter(predicted == "1" & actual == "1")
fn_games <- predictions %>% filter(predicted == "0" & actual == "1")
```

```{r fig.width=12}
predictions_boxplots <- function (fp, tn, tp, fn, col) {
	col_sym <- sym(col)
	
	ggplot() +
		geom_boxplot(data = fp, aes(x = "FP", y = !!col_sym), fill = "orange") +
		geom_boxplot(data = tn, aes(x = "TN", y = !!col_sym), fill = "lightblue") +
		geom_boxplot(data = tp, aes(x = "TP", y = !!col_sym), fill = "lightgreen") +
		geom_boxplot(data = fn, aes(x = "FN", y = !!col_sym), fill = "violet") +
		labs(title = paste(col, "distribution by prediction class"),
			 x = "Outcome class",
			 y = paste(col, "(team1 - team2)")) +
		theme_minimal()
}

predictions_boxplots(fp_games, tn_games, tp_games, fn_games, "kill_diff") +
	predictions_boxplots(fp_games, tn_games, tp_games, fn_games, "tower_diff") +
	predictions_boxplots(fp_games, tn_games, tp_games, fn_games, "dragon_diff") +
	predictions_boxplots(fp_games, tn_games, tp_games, fn_games, "rift_herald_diff") +
	plot_layout(ncol = 2)
```

In two predictors used in the decision tree, `kill_diff` and `dragon_diff`, the
FP prediction class has distribution wide enough to overlap with TP. Other two
diffs have similar distributions. And distributions of FN have overlap with TN,
which makes the tree classify them as team 1 lost, even though it won.

```{r fig.width=12}
predictions_proportions <- function(fp, tn, tp, fn, col) {
	col_sym <- sym(col)
	
	bind_rows(
		fp %>% count(!!col_sym) %>% mutate(class = "FP", Prop = n / sum(n)),
		fn %>% count(!!col_sym) %>% mutate(class = "FN", Prop = n / sum(n)),
		tn %>% count(!!col_sym) %>% mutate(class = "TN", Prop = n / sum(n)),
		tp %>% count(!!col_sym) %>% mutate(class = "TP", Prop = n / sum(n))
	) %>%
		ggplot(aes(x = !!col_sym, y = Prop, fill = class)) +
		geom_col(position = "dodge") +
		scale_y_continuous(labels = scales::percent) +
		labs(title = paste("Proportion of", col, "by prediction class"),
			 x = paste(col, "achieved by"),
			 fill = "prediction class") +
		theme_minimal()
}

predictions_proportions(fp_games, tn_games, tp_games, fn_games, "first_blood") +
	predictions_proportions(fp_games, tn_games, tp_games, fn_games, "first_tower") +
	predictions_proportions(fp_games, tn_games, tp_games, fn_games, "first_dragon") +
	predictions_proportions(fp_games, tn_games, tp_games, fn_games, "first_herald") +
	plot_layout(ncol = 2)
```

If `first_herald` was used, maybe the tree could correctly predict more games, because
FNs had more `first_heralds` then FPs, and vice versa when team 2 captured it.
We will set the predictors to be `kill_diff`, `dragon_diff` and `first_herald`,
but we must also change the `cp` for tree to be able to pickup the new predictor.

```{r}
formula <- team1_won ~ kill_diff + dragon_diff + first_herald
tree_model <- rpart(
	formula = formula,
	data = train_data,
	method = "class",
	cp = 0.001)

predictions <- predict(tree_model, newdata = train_data, type = "class")
train_conf_matrix <- confusionMatrix(data = predictions, reference = train_data$team1_won, positive = "1")

predictions <- predict(tree_model, newdata = test_data, type = "class")
test_conf_matrix <- confusionMatrix(data = predictions, reference = test_data$team1_won, positive = "1")

compare_conf_matrices(train_conf_matrix, test_conf_matrix, c("training", "testing"))
```

This slightly balances the model. We will now fine tune the `cp` parameter on
the tree.

```{r}
set.seed(42069)

train_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final"
)

tune_grid <- expand.grid(cp = seq(from = 0.000001, to = 0.002, by = 0.000005))

tuned_tree_model <- train(
  form = team1_won ~ kill_diff + dragon_diff + first_herald,
  data = train_data,
  method = "rpart",
  trControl = train_control,
  tuneGrid = tune_grid,
  metric = "Accuracy"
)

plot(tuned_tree_model,
     main = "Decision tree performance vs. complexity (cp)",
     xlab = "complexity parameter (cp)",
     ylab = paste(tuned_tree_model$metric))
```

```{r}
predictions <- predict(tuned_tree_model, newdata = test_data)
confusionMatrix(data = predictions, reference = test_data$team1_won, positive = "1")

predictions <- predict(tuned_tree_model, newdata = train_data)
train_conf_matrix <- confusionMatrix(data = predictions, reference = train_data$team1_won, positive = "1")

predictions <- predict(tuned_tree_model, newdata = test_data)
test_conf_matrix <- confusionMatrix(data = predictions, reference = test_data$team1_won, positive = "1")

compare_conf_matrices(train_conf_matrix, test_conf_matrix, c("training", "testing"))
```

The `cp` tuning will not save us here. Even though the `cp` was the best
it could, it provided no significant change to the model's accuracy, which
is the same as with arbitrary chosen `cp = 0.001`, 67.83%

## Random forest

First we will use simplest random forest, using defaults.

```{r}
set.seed(42069)

formula <- team1_won ~ kill_diff + dragon_diff + first_herald

rf_model <- ranger(
	formula = formula,
	data = train_data
)
predictions <- predict(rf_model, data = train_data)
train_conf_matrix <- confusionMatrix(data = predictions$predictions, reference = train_data$team1_won, positive = "1")

predictions <- predict(rf_model, data = test_data)
test_conf_matrix <- confusionMatrix(data = predictions$predictions, reference = test_data$team1_won, positive = "1")

compare_conf_matrices(train_conf_matrix, test_conf_matrix, c("training", "testing"))
```

This model gave more balanced predictions. But the accuracy is very similar to
the best tuned decision tree. Let's see what happens when we let more features
into the model.

```{r}
set.seed(42069)

formula <- team1_won ~ kill_diff + dragon_diff + rift_herald_diff +
  tower_diff + first_blood + first_dragon + first_herald +
  first_tower

rf_model <- ranger(
	formula = formula,
	data = train_data,
)

predictions <- predict(rf_model, data = train_data)
train_conf_matrix <- confusionMatrix(data = predictions$predictions, reference = train_data$team1_won, positive = "1")

predictions <- predict(rf_model, data = test_data)
test_conf_matrix <- confusionMatrix(data = predictions$predictions, reference = test_data$team1_won, positive = "1")

compare_conf_matrices(train_conf_matrix, test_conf_matrix, c("training", "testing"))
```

This is the highest accuracy so far, and the most balanced model yet.
But still there isn't any significant improvement, so we think we have a case
of SISO here.

## AdaBoost

We found that AdaBoost is somewhat of a "next" step in classification trees.
It trains shallow trees sequentially and each tree has an influence on the next
one. We found [StatQuest video](https://www.youtube.com/watch?v=LsK-xG1cLYA)
explaining AdaBoost, and also found a quick
[R tutorial](https://www.geeksforgeeks.org/adaboost-using-caret-package-in-r/)
on how to train AdaBoost model.


```{r}
set.seed(42069)

ada_formula <- team1_won ~ kill_diff + dragon_diff + rift_herald_diff +
  tower_diff + first_blood + first_dragon + first_herald +
  first_tower

model_file_path <- "models/adaboost_tuned.rds"

if (file.exists(model_file_path)) {
	adaboost_model <- readRDS(model_file_path)
} else {
	adaboost_model <- train(
		form = ada_formula,
		data = train_data,
		method = "AdaBoost.M1",
		metric = "Accuracy",
		trControl = trainControl(method = "cv", number = 5) 
	)
}

predictions <- predict(adaboost_model, newdata = train_data)
train_conf_matrix <- confusionMatrix(data = predictions, reference = train_data$team1_won, positive = "1")

predictions <- predict(adaboost_model, newdata = test_data)
test_conf_matrix <- confusionMatrix(data = predictions, reference = test_data$team1_won, positive = "1")

compare_conf_matrices(train_conf_matrix, test_conf_matrix, c("training", "testing"))
```

We are now certain that we have here a case of SISO. The AdaBoost has same
accuracy, but worse balance, then the random forest. We will stop trying
different models, because we have evidence here that our data isn't good enough
for better models.

## Comparions on testing set

Having trained and evaluated a decision tree, a random forest, and an AdaBoost
model, we now compare their performance on the test set:

*   **Accuracy:** Overall proportion of correct predictions.
*   **Sensitivity:** Ability to correctly predict team 1 wins.
*   **Specificity:** Ability to correctly predict Team 1 losses.
*   **Balanced accuracy:** The average of Sensitivity and Specificity, or how the model is "balanced".

Models in comparison table:

1. the initial decision tree (`cp=0.01`, all features),
2. the random forest (all features, default parameters),
3. and the AdaBoost.M1 model (all features, tuned `mfinal` and `maxdepth` implicitly by caret's default grid with `number=5` CV).

| Model         | Accuracy | Sensitivity | Specificity | Balanced accuracy |
| :------------ | :------- | :---------- | :---------- | :---------------- |
| Decision Tree | 0.6689   | 0.7508      | 0.5771      | 0.6639            |
| Random Forest | 0.6806   | 0.6890      | 0.6712      | 0.6801            |
| AdaBoost.M1   | 0.6812   | 0.7245      | 0.6327      | 0.6786            |

1.  **Accuracy:** All three models achieved similar overall accuracy,
	falling within a slim range of approximately 67% to 68.1%.
	The AdaBoost.M1 model shows the highest overall accuracy (0.6812), but the
	difference compared to the random forest (0.6806) is minimal. This suggests
	that with the current feature set, the models are hitting a similar performance
	ceiling.
2.  **Balance:** This is where the models show clearer differences:
    *   The initial **Decision Tree** had the highest Sensitivity (75.1%), meaning
	    it was best at identifying actual Team 1 wins. However, it had the lowest
	    Specificity (57.7%), failing to identify Team 1 losses, which indicates
	    overfitting.
    *   The **Random Forest** achieved the most balanced performance
	    (Sensitivity 68.9%, Specificity 67.1%). Its Sensitivity was lower than
	    the decision tree's, but its Specificity was significantly higher
	    Overall it had more balanced predictions.
    *   The **AdaBoost.M1** has higher Sensitivity (72.5%) than the Random Forest, but
	    lower Specificity (63.3%). Like the initial Decision Tree, it shows a
	    significant bias towards predicting Team 1 wins, making more FP errors
	    than FNs.

Based on our evaluations, the **Random Forest with all features** appears to be
the best model from those tested. While its overall accuracy is only slightly
higher than the AdaBoost.M1 model, it's more **balanced** (similar Sensitivity and Specificity),
making it a more reliable across both winning and losing scenarios for team 1.

The consistent plateau in accuracy around 67-68% across different model types
suggests that the current set of early-game features (`_diff` and `first_`) may
have reached their limit in terms of predictive power for the final game outcome.
This supports our earlier conclusion about this being a case of SISO with the
given features. Also, games of League of Legends can be won after early
game, early game state doesn't have to dictate the end game result.


## Conclusion

In this project, we aimed to predict the winner of professional League of Legends
games based on events occurring within the first 12 minutes. We engineered a
dataset capturing key early-game metrics, including objective differences
(`_diff`) and the teams that secured the first significant events (`first_`).

We explored three classification model types: a single Decision Tree, a Random
Forest, and an AdaBoost.M1 model. Our analysis involved training these models
on a training dataset and evaluating their performance using confusion matrices
and key metrics on a test set.

Across all models tested with our current feature set, we observed a consistent
overall accuracy falling within a range of 67% to 68.1%.

The models had interesting differences in their predictive balance:
*   The initial **Decision Tree** showed high Sensitivity (good at predicting wins)
	but low Specificity (poor at predicting losses), indicating a bias towards
	the positive class.
*   The **Random Forest**, achieved the
	most balanced performance, with similar Sensitivity and Specificity,
	suggesting a more robust identification of both winning and losing scenarios
	for Team 1.
*   The **AdaBoost.M1** model leaned towards higher Sensitivity (predicting wins)
	compared to the Random Forest, but at the cost of lower Specificity (predicting losses),
	resulting in a biased error distribution similar to the single Decision Tree.

The consistent plateau in performance around 67-68% accuracy across different model
types and tuning efforts suggests that the primary limitation is the information
contained within our current set of early-game features. This supports our conclusion
that we are encountering a SISO scenario, where the predictive power is constrained
by the available input features rather than the specific choice of model within this class.

Furthermore, predicting the final outcome of a complex, dynamic game like League
of Legends based *only* on early-game indicators is inherently challenging.
Significant events, strategic decisions, and individual player performance fluctuations
occur throughout the mid and late game, which are not captured by our 12-minute snapshot.

While our models can predict the winner with an accuracy significantly better than
random chance (~53%), achieving substantially higher predictive performance would
mean incorporating more comprehensive early-game features, such as Gold Difference
and Experience Difference, or potentially incorporating data from later stages of
the game. Based on our findings, with the current feature set, the Random Forest
model offered the best balance of predictive power and error distribution.

