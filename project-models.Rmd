---
title: "Models"
author: "Lukáš Častven, Michal Kilian"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(rpart)
library(rpart.plot)
library(caret)
source("./dataset.R")

N <- 12 * 60
dataset <- early_game_dataset(player_stats, metadata, events, champs, N)

color1 <- "plum"
color2 <- "#A0DDA1"
```

## Decision trees

```{r}
data <- dataset %>% mutate(team1_won = factor(team1_won))

set.seed(42069) 
train_indices <- createDataPartition(data$team1_won, p = 0.8, list = FALSE, times = 1)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

simple_tree_model <- rpart(
	formula = team1_won ~ .,
	data = train_data,
	method = "class",
	control = rpart.control(maxdepth = 9, cp = 0.01)
)

predictions <- predict(simple_tree_model, newdata = test_data, type = "class")

expected_levels <- levels(data$team1_won)
predictions_factor <- factor(predictions, levels = expected_levels)
actual_factor <- factor(test_data$team1_won, levels = expected_levels)
						  
conf_matrix <- confusionMatrix(predictions_factor, actual_factor)
print(conf_matrix)
```



```{r fig.width=12}
rpart.plot(
	simple_tree_model,
	box.palette = c(color2, color1), # Color nodes: Team2 Wins (0), Team1 Wins (1)
	main = "Predicting winner of a game by early game",
)
```

```{r}
data <- dataset %>% mutate(team1_won = factor(team1_won))

set.seed(42069) # Set seed for reproducibility of the split
train_indices <- createDataPartition(data$team1_won, p = 0.8, list = FALSE, times = 1)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

train_control <- trainControl(method = "cv", number = 10)

tune_grid <- expand.grid(
  cp = seq(from = 0.0001, to = 0.002, by = 0.0001)
)

tuned_tree_model <- train(
  team1_won ~ .,
  data = train_data,
  method = "rpart",
  trControl = train_control,
  tuneGrid = tune_grid,
  metric = "Accuracy"
)

print(paste("Best cp value:", tuned_tree_model$bestTune$cp))
plot(tuned_tree_model)
```

```{r}
predictions_tuned <- predict(tuned_tree_model, newdata = test_data)

expected_levels <- levels(data$team1_won)
predictions_tuned_factor <- factor(predictions_tuned, levels = expected_levels)
actual_factor <- factor(test_data$team1_won, levels = expected_levels)

conf_matrix_tuned <- confusionMatrix(predictions_tuned_factor, actual_factor)
print(conf_matrix_tuned)
```

We trained a random forest, but it took 26 minutes and the results were,
little better than the decision tree.

The result 

```
[1] "Training finished in: 26.83156 mins"
Random Forest 

29960 samples
   11 predictor
    2 classes: '0', '1' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 29960, 29960, 29960, 29960, 29960, 29960, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
   2    0.6807033  0.3579158
   8    0.6609411  0.3185253
  15    0.6562741  0.3094617

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 2.
```


















