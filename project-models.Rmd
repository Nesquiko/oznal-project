---
title: "Models"
author: "Lukáš Častven, Michal Kilian"
date: "`r Sys.Date()`"
output: html_document
---

Based on these EDA findings, as our first model, we choose simple decision trees.
This model choice is supported by the EDA which identified some non-linear relationships
(e.g., the threshold effect of securing `first_` objectives) and mixed data types
(numeric `_diff`, `_champdiff`, `time_first_*`, categorical `first_*`). While features
like `time_first_*` and `_champdiff` showed limited individual predictive power.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(tidyverse)
library(tidymodels)
library(rpart)
library(rpart.plot)
library(caret)
source("./dataset.R")

N <- 12 * 60
dataset <- early_game_dataset(player_stats, metadata, events, champs, N) %>%
	mutate(team1_won = factor(team1_won))

set.seed(42069)
train_indices <- createDataPartition(dataset$team1_won, p = 0.75, list = FALSE, times = 1)
train_data <- dataset[train_indices, ]
test_data <- dataset[-train_indices, ]

color1 <- "plum"
color2 <- "#A0DDA1"
```

## Decision trees

```{r}
decision_tree <- function(formula, cp = 0.01) {
	# https://rdrr.io/cran/rpart/man/rpart.html
	tree_model <- rpart(
		formula = formula,
		data = train_data,
		method = "class",
		cp = cp
	)
	
	predictions <- predict(tree_model, newdata = test_data, type = "class")
	conf_matrix <- confusionMatrix(data = predictions, reference = test_data$team1_won)
	return (list(model = tree_model, conf_matrix = conf_matrix))
}

plot_decision_tree <- function(tree_model) {
	rpart.plot(
		tree_model,
		box.palette = c(color2, color1),
		main = "Predicting winner of a game by early game",
	)
}
```


We first try all features that shows promise in EDA (`first_` and diffs).

```{r}
formula <- team1_won ~ kill_diff + dragon_diff + rift_herald_diff +
  tower_diff + first_blood + first_dragon + first_herald +
  first_tower

all_tree <- decision_tree(formula)
plot_decision_tree(all_tree$model)
all_tree$conf_matrix$overall['Accuracy']
all_tree$conf_matrix
```

With the default settings it chose `kill_diff` and `dragon_diff`, which has
accuracy `r all_tree$conf_matrix$overall['Accuracy']`. Next we will try to tune
the `cp` parameter, which sets threshold for how much a tree split should improve
the model (default was 0.01).

```{r}
set.seed(42069)

train_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final"
)

tune_grid <- expand.grid(cp = seq(from = 0.000001, to = 0.0015, by = 0.000005))

tuned_tree_model <- train(
  form = formula,
  data = train_data,
  method = "rpart",
  trControl = train_control,
  tuneGrid = tune_grid,
  metric = "Accuracy"
)

plot(tuned_tree_model,
     main = "Decision tree performance vs. complexity (cp)",
     xlab = "complexity parameter (cp)",
     ylab = paste(tuned_tree_model$metric))
```

```{r}
plot_decision_tree(tuned_tree_model$finalModel)
tuned_tree_model$bestTune
tuned_tree_model$results[rownames(tuned_tree_model$bestTune),][['Accuracy']]
```

The best tuned model had `cp = 0.000391` and accuracy `r tuned_tree_model$results[rownames(tuned_tree_model$bestTune),][['Accuracy']]`,
which is around 2% better. Not much of an improvement.


```{r}
logreg_model <- logistic_reg() %>%
	set_engine("glm") %>%
	set_mode("classification") %>%
	fit(team1_won ~ kill_diff, data = train_data)

predictions <- predict(logreg_model, new_data = test_data, type = "class")

conf_matrix <- confusionMatrix(data = predictions$.pred_class, reference = test_data$team1_won)

print(conf_matrix)
```




















We trained a random forest, but it took 26 minutes and the results were,
little better than the decision tree.

The result 

```
[1] "Training finished in: 26.83156 mins"
Random Forest 

29960 samples
   11 predictor
    2 classes: '0', '1' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 29960, 29960, 29960, 29960, 29960, 29960, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
   2    0.6807033  0.3579158
   8    0.6609411  0.3185253
  15    0.6562741  0.3094617

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 2.
```
